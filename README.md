# LinkedIn Scraper Extension

Extension Chrome pour scraper automatiquement vos posts LinkedIn et les sauvegarder dans une base de donn√©es Supabase.

## Fonctionnalit√©s

- ‚úÖ Scraping automatique des posts LinkedIn lors de la navigation
- ‚úÖ Sauvegarde des donn√©es : texte, date, URL, likes, commentaires, impressions
- ‚úÖ Authentification via Supabase Auth
- ‚úÖ Rescraping automatique tous les 7 jours pour les posts < 45 jours
- ‚úÖ API pour tracker les √©v√©nements (clics, RDV)
- üöß Int√©gration Meilisearch pour la recherche
- üöß Dashboard Next.js

## Stack Technique

- **Extension**: Plasmo Framework + TypeScript
- **Base de donn√©es**: PostgreSQL (Supabase)
- **Auth**: Supabase Auth
- **API**: Supabase Edge Functions
- **Cron**: Supabase Cron Jobs
- **Recherche**: Meilisearch (√† venir)
- **Dashboard**: Next.js (√† venir)

## Installation

### 1. Configuration Supabase

1. Cr√©ez un projet sur [Supabase](https://supabase.com)
2. Copiez vos cl√©s API dans `.env.local`:
   ```
   PLASMO_PUBLIC_SUPABASE_URL=votre-url-supabase
   PLASMO_PUBLIC_SUPABASE_ANON_KEY=votre-anon-key
   ```
3. Ex√©cutez les migrations :
   ```bash
   npx supabase db push
   ```
4. D√©ployez les Edge Functions :
   ```bash
   npx supabase functions deploy rescrape-posts
   npx supabase functions deploy track-event
   ```

### 2. Installation de l'extension

1. Installez les d√©pendances :
   ```bash
   npm install
   ```

2. Lancez le mode d√©veloppement :
   ```bash
   npm run dev
   ```

3. Chargez l'extension dans Chrome :
   - Allez dans `chrome://extensions/`
   - Activez le "Mode d√©veloppeur"
   - Cliquez sur "Charger l'extension non empaquet√©e"
   - S√©lectionnez le dossier `build/chrome-mv3-dev`

## Utilisation

1. Cliquez sur l'ic√¥ne de l'extension et connectez-vous avec votre compte
2. Naviguez sur LinkedIn - vos posts seront automatiquement scrap√©s
3. Les donn√©es sont synchronis√©es en temps r√©el avec Supabase

## Structure de la base de donn√©es

### Table `posts`
- `id`: UUID (PK)
- `user_id`: UUID (FK vers auth.users)
- `text`: Contenu du post
- `post_url`: URL unique du post
- `posted_at`: Date de publication
- `likes`: Nombre de likes
- `comments`: Nombre de commentaires
- `impressions`: Nombre d'impressions
- `last_scraped_at`: Derni√®re date de scraping

### Table `events`
- `id`: UUID (PK)
- `post_id`: UUID (FK vers posts)
- `type`: Type d'√©v√©nement (click/rdv)
- `metadata`: Donn√©es additionnelles (JSONB)
- `created_at`: Date de l'√©v√©nement

## API Endpoints

### Track Event
```bash
POST /functions/v1/track-event
Authorization: Bearer <user-token>
{
  "post_id": "uuid",
  "type": "click" | "rdv",
  "metadata": {}
}
```

### Rescrape Posts (Cron)
```bash
POST /functions/v1/rescrape-posts
Authorization: Bearer <service-role-key>
```

## D√©veloppement

```bash
# Mode dev
npm run dev

# Build pour production
npm run build

# Package pour distribution
npm run package
```

## Roadmap

- [ ] Am√©liorer la d√©tection des impressions (n√©cessite ouverture du d√©tail)
- [ ] Int√©grer Meilisearch pour la recherche full-text
- [ ] Cr√©er le dashboard Next.js
- [ ] Ajouter des graphiques d'analytics
- [ ] Export des donn√©es (CSV, PDF)
- [ ] Notifications pour les posts performants